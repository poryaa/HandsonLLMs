{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2849cf",
   "metadata": {},
   "source": [
    "# Chapter04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cdb431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer,T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "model =T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Is the following sentence positive or negative?\"\n",
    "data = data.map(lambda example: {\"t5\": prompt + example[\"text\"]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c994e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"Create and print the classification report\"\"\"\n",
    "    performance = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
    "    )\n",
    "    print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a24ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "\n",
    "y_pred = []\n",
    "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")), total=len(data[\"test\"])):\n",
    "    text = output[0][\"generated_text\"]\n",
    "    y_pred.append(0 if text == \"negative\" else 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce7b478",
   "metadata": {},
   "source": [
    "# Chapter 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883593d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"maartengr/arxiv_nlp\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ffce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = dataset[\"Abstracts\"]\n",
    "titles = dataset[\"Titles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa183fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-small\")\n",
    "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3152932",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_components= 5, min_dist=0.0, metric=\"cosine\", random_state=42)\n",
    "reduced_embeddings =umap_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c7943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=50, metric=\"euclidean\", cluster_selection_method=\"eom\").fit(reduced_embeddings)\n",
    "clusters =hdbscan_model.labels_\n",
    "len(set(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56faf155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reduce 384-dimensional embeddings to 2 dimensions for easier visualization\n",
    "reduced_embeddings = UMAP(\n",
    "    n_components=2, min_dist=0.0, metric='cosine', random_state=42\n",
    ").fit_transform(embeddings)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\n",
    "df[\"title\"] = titles\n",
    "df[\"cluster\"] = [str(c) for c in clusters]\n",
    "\n",
    "# Select outliers and non-outliers (clusters)\n",
    "clusters_df = df.loc[df.cluster != \"-1\", :]\n",
    "outliers_df = df.loc[df.cluster == \"-1\", :]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot outliers and non-outliers seperately\n",
    "plt.scatter(outliers_df.x, outliers_df.y, alpha=0.05, s=2, c=\"grey\")\n",
    "plt.scatter(\n",
    "    clusters_df.x, clusters_df.y, c=clusters_df.cluster.astype(int),\n",
    "    alpha=0.6, s=2, cmap='tab20b'\n",
    ")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9431fa",
   "metadata": {},
   "source": [
    "# Chapter 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9aa112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"cuda\",\n",
    "                                             torch_dtype=\"auto\",\n",
    "                                             trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                return_full_text=False,\n",
    "                max_new_tokens=500,\n",
    "                do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698061a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check pipeline info\n",
    "type(pipe)\n",
    "# --- IGNORE ---\n",
    "dir(pipe)\n",
    "pipe.task\n",
    "pipe.model\n",
    "pipe.tokenizer\n",
    "pipe.model.config\n",
    "pipe.model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt\n",
    "messages = [{\"role\": \"user\",\n",
    "             \"content\": \"Create a funny joke about chickens.\"}]\n",
    "#generate the output\n",
    "output = pipe(messages)\n",
    "output[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(messages, do_sample=True, temperature=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(messages, do_sample=True, top_p=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_prompt =[{\"role\":\"user\",\n",
    "                  \"content\": \" Create a name and slogan for a chatbot that leverages LLMs.\"}]\n",
    "outputs = pipe(product_prompt)\n",
    "product_description = outputs[0][\"generated_text\"]\n",
    "print(product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_prompt=[{\"role\":\"user\",\n",
    "               \"content\": f\"Generate a very short sales pitch for the following product: '{product_description}'\"}]\n",
    "outputs = pipe(sales_prompt)\n",
    "sales_pitch = outputs[0][\"generated_text\"]\n",
    "print(sales_pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b34046",
   "metadata": {},
   "source": [
    "# Chapter 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ae385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm_model = OllamaLLM(model=\"tinyllama:latest\",\n",
    "                  n_gpu_layers=-1,\n",
    "                  max_tokens=20,\n",
    "                  n_ctx=2048,\n",
    "                  seed=42,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20399212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a great assistant, Marten! Thank you for using my knowledge and insights to make your life easier. I appreciate it more than words could express. It's always nice when people appreciate the work we do for them. Have a great day!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.invoke(\"Hi! My name is Marten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff638b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! The formula for computing the sum of two integers (i.e. 1+1) is simply:\\n\\n1+1 = 2\\n\\nSo the result will be:\\n\\n2\\n\\nHope that helps! Let me know if you have any other questions or need further assistance.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"just show me the answer of: {input_prompt}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template,\n",
    "                        input_variable=[\"input_prompt\"])\n",
    "\n",
    "basic_chain = prompt | llm_model\n",
    "basic_chain.invoke({\"input_prompt\":\" Hi! what is 1+1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "386a8044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Maya strode through the streets of her small town, she felt as if the world had turned upside down. She couldn't shake the feeling that something was missing in her life, a familiar sense of emptiness that had been plaguing her since her mother passed away years ago. But even as she searched for answers, she knew that moving to a new city would be the beginning of her journey towards finding her mom.\n",
      "\n",
      "Maya never thought that moving to a big city like New York could be the solution, but something about the vibrant streets and bustling crowds drew her in. She set out on a solo adventure, determined to find her mom wherever she might be, with no expectation or plans. As she made her way through the crowded subways, the hum of voices and chatter echoing around her, Maya felt a sense of determination that had eluded her for far too long.\n",
      "\n",
      "It wasn't until she stumbled upon a small coffee shop in an unfamiliar part of town that she realized just how far she had come. The owner greeted her with a warm smile and showed her to a secluded corner near the window, where she sat at a table alone. She sipped on her coffee as she waited for the perfect moment to approach the owner, a woman in her 30s with tousled hair and bright eyes that seemed to see deep into Maya's heart.\n",
      "\n",
      "At first, Maya didn't know what to say, unsure if this stranger could truly help her. But as she leaned over the table, the waitress turned to face her, a gentle smile on her face. \"I heard your mom's name was Linda,\" she said softly, her gaze drifting from Maya's to the coffee shop as if it were a living thing. \"My name is Maya.\"\n",
      "\n",
      "Maya felt tears prick at the corners of her eyes, but she managed to hold back, unsure of how to proceed. But when the woman asked what had happened to her mom and why she needed to find her, Maya's voice cracked slightly. She spoke slowly and deliberately, trying to convey just how deeply her mother had meant to her life.\n",
      "\n",
      "\"I know it must seem like a long shot,\" the woman said, pausing for a moment before continuing in Maya's stead. \"But sometimes, when you really have nothing else left to do but search, the universe works in mysterious ways.\"\n",
      "\n",
      "Maya felt her heart skip a beat as she listened to the stranger's words, realizing that something was happening within her that she couldn't quite explain. She felt like there was a part of her that had been missing ever since her mom left her, and now, finally, it seemed that maybe she might get answers.\n",
      "\n",
      "Over the next few days, Maya returned to the coffee shop frequently, talking with the owner about Linda and the possibility of finding her daughter. They shared stories, laughed, and even got to know each other a little bit. And in that time, Maya began to feel like she might be close to something. She realized that perhaps this was what she'd been searching for all along, even though it seemed impossible.\n",
      "\n",
      "As the days turned into weeks, Maya felt more confident than ever before. She knew that it wouldn't be easy to find Linda, but she also knew that no matter what happened, she would do everything in her power to make sure that she was okay. When she heard back from the owner about a possible lead, she took the opportunity to go to visit her. And when she finally met Linda and got the chance to ask how things were going for her daughter, she felt as if the world had swung into a new orbit altogether.\n",
      "\n",
      "Maya knew that there would be challenges ahead, but she also knew that this might just be what she needed to get through. The journey wasn't easy, and at times it seemed like nothing could go right. But the moments with Linda were special, and Maya felt as though she had been given a gift of sorts. As she stood outside the coffee shop one last time, she knew that whatever the future held, she was ready to face it head-on. And for the first time in years, Maya felt like she could truly say that wherever life might take her next, it would be a step forward, no matter what was on the other side.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1) Title\n",
    "title_prompt = PromptTemplate(\n",
    "    template=\"Create a title for a story about {summary}. Only return the title.\",\n",
    "    input_variables=[\"summary\"],\n",
    ")\n",
    "\n",
    "def extract_text(output):\n",
    "    # Handles ChatMessage / AIMessage / plain strings\n",
    "    return getattr(output, \"content\", str(output))\n",
    "\n",
    "def title_chain(input_dict):\n",
    "    title = llm_model.invoke(title_prompt.format(**input_dict))\n",
    "    return {\n",
    "        \"summary\": input_dict[\"summary\"],\n",
    "        \"title\": extract_text(title).strip(),\n",
    "    }\n",
    "\n",
    "# 2) Character\n",
    "character_prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"Describe the main character of a story about {summary} \"\n",
    "        \"with the title of {title}. Use only 2 sentences.\"\n",
    "    ),\n",
    "    input_variables=[\"summary\", \"title\"],\n",
    ")\n",
    "\n",
    "def character_chain(input_dict):\n",
    "    character = llm_model.invoke(character_prompt.format(**input_dict))\n",
    "    return {\n",
    "        \"summary\": input_dict[\"summary\"],\n",
    "        \"title\": input_dict[\"title\"],\n",
    "        \"character\": extract_text(character).strip(),\n",
    "    }\n",
    "\n",
    "# 3) Story\n",
    "story_prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"Create a story about {summary}. The main character is {character} \"\n",
    "        \"with the title of {title}. Only return the story and it cannot be \"\n",
    "        \"longer than one paragraph.\"\n",
    "    ),\n",
    "    input_variables=[\"summary\", \"title\", \"character\"],\n",
    ")\n",
    "\n",
    "def story_chain(input_dict):\n",
    "    story = llm_model.invoke(story_prompt.format(**input_dict))\n",
    "    return extract_text(story).strip()\n",
    "\n",
    "# Run the pipeline\n",
    "input_data = {\"summary\": \"A girl that lost her mother.\"}\n",
    "result = story_chain(character_chain(title_chain(input_data)))\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4b555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".LLM_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
